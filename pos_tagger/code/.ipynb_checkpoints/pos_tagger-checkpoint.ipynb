{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing all libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting gpu\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'keras_model.py', 'pos_tagger.ipynb']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(file_name):\n",
    "    \n",
    "    '''prepareing data to required form'''\n",
    "    \n",
    "    count_tagged_words = 0\n",
    "    tagged_sentence = []\n",
    "    with open(os.path.join('../input',file_name)) as f: \n",
    "        i  =0\n",
    "        for line in f:\n",
    "            if line.strip()=='':\n",
    "                pass\n",
    "            else:\n",
    "                tagged_sent = [nltk.tag.str2tuple(t) for t in line.split()]\n",
    "                tagged_sentence.append(tagged_sent) \n",
    "                count_tagged_words +=  len(tagged_sent)\n",
    "\n",
    "    print('\\ntagged sentence in %s  %s '%(file_name,len(tagged_sentence)))\n",
    "    print('tagged words in %s  %s '%(file_name,count_tagged_words))\n",
    "    return tagged_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_features(sentence, loc):\n",
    "    \n",
    "    '''defining features'''\n",
    "    \n",
    "    return {\n",
    "        'word': sentence[loc],\n",
    "        'is_first': loc == 0,\n",
    "        'is_last': loc == len(sentence) - 1,\n",
    "        'is_all_caps': sentence[loc].upper() == sentence[loc],\n",
    "        'is_all_lower': sentence[loc].lower() == sentence[loc],\n",
    "        'first_prefix': sentence[loc][0],\n",
    "        'second_prefix': sentence[loc][:2],\n",
    "        'third_prefix': sentence[loc][:3],\n",
    "        'first_suffix': sentence[loc][-1],\n",
    "        'second_suffix': sentence[loc][-2:],        \n",
    "        'suffix-3': sentence[loc][-3:],\n",
    "        'is_capitalized': sentence[loc][0].upper() == sentence[loc][0],\n",
    "        'prev_word': '' if loc == 0 else sentence[loc - 1],\n",
    "        'next_word': '' if loc == len(sentence) - 1 else sentence[loc + 1],\n",
    "        'hyphen_present': '-' in sentence[loc],\n",
    "        'is_numeric': sentence[loc].isdigit(),\n",
    "        'capitals_inside': sentence[loc][1:].lower() != sentence[loc][1:]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_dataset(tagged_sentences):\n",
    "    \n",
    "    '''\n",
    "    transforming data\n",
    "    '''\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        for i in range(len(tagged)):\n",
    "            X.append(custom_features(untag(tagged), i))\n",
    "            y.append(tagged[i][1])\n",
    "    \n",
    "    c = list(zip(X, y))\n",
    "\n",
    "    random.shuffle(c)\n",
    "\n",
    "    X, y = zip(*c)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decision_tree(X,y):\n",
    "    \n",
    "    '''defining decision tree'''\n",
    "    \n",
    "    decision_clf = Pipeline([('vectorizer', DictVectorizer(sparse=False)),('classifier', DecisionTreeClassifier(criterion='entropy'))])\n",
    "    decision_clf.fit(X, y)\n",
    "    print('Training completed')\n",
    "    return decision_clf\n",
    "\n",
    "def random_tree(X, y):\n",
    "    \n",
    "    '''random_classifier'''\n",
    "    \n",
    "    random_clf =  Pipeline([('vectorizer', DictVectorizer(sparse=False)),('classifier', RandomForestClassifier(n_estimators=200, max_depth=50,random_state=0))])\n",
    "     \n",
    "    random_clf.fit(X, y)\n",
    "    print('Training completed')\n",
    "    return random_clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tagged sentence in train.txt  10388 \n",
      "tagged words in train.txt  228333 \n",
      "\n",
      "number of sentences in training set 7791,\n",
      " number of sentences in test set 2597\n",
      "\n",
      "number of words in training set = 165816\n",
      "number of words in test set = 62517\n",
      "\n",
      "tagged sentence in test.tag  29 \n",
      "tagged words in test.tag  892 \n",
      "number of words in eval set = 892\n"
     ]
    }
   ],
   "source": [
    "# preparing training datasety\n",
    "train_file ='train.txt'\n",
    "training_set =  prepare_data(train_file)\n",
    "\n",
    "# dividing dataset into training set and test set \n",
    "cutoff = int(.75 * len(training_set))\n",
    "training_sentences = training_set[:cutoff]\n",
    "test_sentences = training_set[cutoff:]\n",
    "print('\\nnumber of sentences in training set %s,\\n number of sentences in test set %s'%(len(training_sentences),len(test_sentences))) \n",
    "\n",
    "# Transforming dataset\n",
    "X_train, y_train = transform_to_dataset(training_sentences)\n",
    "print('\\nnumber of words in training set = %s'%(len(X_train)))\n",
    "X_test, y_test = transform_to_dataset(test_sentences)\n",
    "print('number of words in test set = %s'%(len(X_test)))\n",
    "\n",
    "eval_file ='test.tag' \n",
    "eval_set =  prepare_data(eval_file)\n",
    "X_eval, y_eval = transform_to_dataset(eval_set)\n",
    "\n",
    "print('number of words in eval set = %s'%(len(X_eval)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# due to less machine computation power training on less data\n",
    "cut_off = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed\n",
      "Accuracy  over test set 0.82859062335\n",
      " stats---- over evalvation set---\n",
      "accuracy 0.862107623318, precision 0.871314775005, recall_score 0.862107623318,f1_score 0.859633559493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudeep/anaconda3/envs/sudeepenv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/sudeep/anaconda3/envs/sudeepenv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/sudeep/anaconda3/envs/sudeepenv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/sudeep/anaconda3/envs/sudeepenv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training decision tree\n",
    "\n",
    "decision_clf  = decision_tree(X_train[:cut_off],y_train[:cut_off])\n",
    "print(\"Accuracy  over test set\", decision_clf.score(X_test,y_test))\n",
    "\n",
    "\n",
    "print(\" stats---- over evalvation set---\")\n",
    "\n",
    "y_predicted = decision_clf.predict(X_eval)\n",
    "accuracy  = accuracy_score(y_eval, y_predicted)\n",
    "precision  = precision_score(y_eval, y_predicted,average = 'weighted')\n",
    "recall_score  = recall_score(y_eval, y_predicted,average = 'weighted')\n",
    "f1_score  = f1_score(y_eval, y_predicted,average = 'weighted')\n",
    "print('accuracy %s, precision %s, recall_score %s,f1_score %s'%(accuracy,precision,recall_score,f1_score))\n",
    "\n",
    "\n",
    "\n",
    "#WRITING FILE\n",
    "accuracy_file = open(os.path.join('../output','accuracy.txt'),'a+')\n",
    "accuracy_file.write('---decision_tree---\\n\\n')\n",
    "accuracy_file.write('accuracy = '+ str(accuracy)+\"\\n\")\n",
    "accuracy_file.write('precision = '+ str(precision)+\"\\n\")\n",
    "accuracy_file.write('recall_score = '+ str(recall_score)+\"\\n\")\n",
    "accuracy_file.write('f1_score = '+ str(f1_score)+\"\\n\\n\\n\")\n",
    "accuracy_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed\n",
      "Accuracy  over test set 0.792520434442\n",
      "Accuracy  over eval set 0.834080717489\n"
     ]
    }
   ],
   "source": [
    "# training random_tree \n",
    "\n",
    "random_clf  = random_tree(X_train[:cut_off],y_train[:cut_off])\n",
    "print(\"Accuracy  over test set\", random_clf.score(X_test,y_test))\n",
    "\n",
    "print(\"Accuracy  over eval set\", random_clf.score(X_eval,y_eval))\n",
    "\n",
    "#WRITING FILE\n",
    "accuracy_file = open(os.path.join('../output','accuracy.txt'),'a+')\n",
    "accuracy_file.write('---random_tree---\\n\\n')\n",
    "accuracy_file.write('accuracy = '+ str(accuracy)+\"\\n\")\n",
    "accuracy_file.write('precision = '+ str(precision)+\"\\n\")\n",
    "accuracy_file.write('recall_score = '+ str(recall_score)+\"\\n\")\n",
    "accuracy_file.write('f1_score = '+ str(f1_score)+\"\\n\\n\\n\")\n",
    "accuracy_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building deep network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#import keras model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "%aimport keras_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               20716544  \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 207)               106191    \n",
      "=================================================================\n",
      "Total params: 20,824,783\n",
      "Trainable params: 20,823,759\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc (zmq/backend/cython/message.c:4294)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54537 samples, validate on 7791 samples\n",
      "Epoch 1/14\n"
     ]
    }
   ],
   "source": [
    "model = keras_model.deep_keras_model(X_train[:cutoff], y_train[:cutoff],X_test[:300], y_test[:300],X_eval,y_eval,epochs = 14)\n",
    "model_json = model_glove.to_json()\n",
    "model.model.save(os.path.join('../output','model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X_predict):\n",
    "    \n",
    "    '''prediction function'''\n",
    "    \n",
    "    file = open(\"label_encoder.obj\",'rb')\n",
    "    label_encoder = pickle.load(file)\n",
    "\n",
    "    file = open(\"dict_vectorizer.obj\",'rb')\n",
    "    dict_vectorizer = pickle.load(file)\n",
    "    \n",
    "    prediction_test = model.predict(X_predict)\n",
    "    predictions_test = label_encoder.inverse_transform(prediction_test)\n",
    "    return predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = 'res.txt'\n",
    "\n",
    "res = open(os.path.join('../output',file_name),'a+')\n",
    "result = []\n",
    "with open(file_name) as f: \n",
    "    for line in f:\n",
    "        if line.strip()=='':\n",
    "            pass\n",
    "        else:\n",
    "            tagged_sent = [nltk.tag.str2tuple(t) for t in line.split()]\n",
    "            X_test_, y_test_ = transform_to_dataset([tagged_sent])\n",
    "            X_predict = dict_vectorizer.transform(X_test_)\n",
    "            predicted = predict(X_predict)\n",
    "\n",
    "            line_ = list(zip(line.split(), predicted))\n",
    "            result.append([str(i[0]+'/'+i[1]) for i in line_])\n",
    "            str_ = ''\n",
    "            for i in range(len(line_)):\n",
    "                \n",
    "                str_ + = line_[i][0] +'/'+line_[i][1]\n",
    "            \n",
    "# print(result)\n",
    "res.write(str(result))\n",
    "res.close()\n",
    "f.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sudeepenv]",
   "language": "python",
   "name": "conda-env-sudeepenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
